{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T_Xc9sl1mGz",
        "outputId": "1fa02672-79a9-472d-cc7c-bc8b0a546902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import os"
      ],
      "metadata": {
        "id": "NlOHeCBy2keY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, dtype='float32')\n",
        "    y_pred = K.cast(y_pred, dtype='float32')\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, dtype='float32')\n",
        "    y_pred = K.cast(y_pred, dtype='float32')\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "    return K.clip(f1, 0, 1)  # 确保 F1-score 在 [0,1] 之间\n"
      ],
      "metadata": {
        "id": "YVp-thyg1nz1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_embedding(dataset_path, tokenizer, batch_size=32):\n",
        "    dataset = pd.read_csv(dataset_path)[[\"tweet\", \"sarcastic\"]]\n",
        "    dataset = dataset.dropna()\n",
        "\n",
        "    tokenized_tweets = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet)) for tweet in dataset['tweet']]\n",
        "\n",
        "    tweets_with_len = [[tweet, dataset['sarcastic'].iloc[i], len(tweet)] for i, tweet in enumerate(tokenized_tweets)]\n",
        "    random.Random(42).shuffle(tweets_with_len)\n",
        "\n",
        "    tweets_with_len.sort(key=lambda x: x[2])  # 按长度排序\n",
        "    sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]\n",
        "\n",
        "    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.int32))\n",
        "\n",
        "    return processed_dataset.padded_batch(batch_size, padded_shapes=((None,), ()))\n",
        "\n"
      ],
      "metadata": {
        "id": "GUhL-0SZ11vg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "def prepare_datasets(train_path, test_path):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # 读取数据集\n",
        "    train_df = pd.read_csv(train_path)[[\"tweet\", \"sarcastic\"]].dropna()\n",
        "    test_df = pd.read_csv(test_path)[[\"tweet\", \"sarcastic\"]].dropna()\n",
        "\n",
        "    # Tokenize 数据\n",
        "    train_encodings = tokenizer(list(train_df[\"tweet\"]), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(list(test_df[\"tweet\"]), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # 创建 TensorFlow 数据集\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_df[\"sarcastic\"].values)).batch(32)\n",
        "    test_data = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_df[\"sarcastic\"].values)).batch(32)\n",
        "\n",
        "    return train_data, test_data, tokenizer\n"
      ],
      "metadata": {
        "id": "qbqiSfft3che"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_path = \"/content/Train_Dataset.csv\"\n",
        "test_path = \"/content/Test_Dataset.csv\"\n",
        "\n",
        "# 运行数据准备\n",
        "train_data, test_data, tokenizer = prepare_datasets(train_path, test_path)\n"
      ],
      "metadata": {
        "id": "G40SDqgN22f8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_MODEL(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"cnn_model\"):\n",
        "        super(CNN_MODEL, self).__init__(name=name)\n",
        "\n",
        "        self.embedding = layers.Embedding(vocabulary_size, embedding_dimensions)\n",
        "\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # 只提取 input_ids，忽略其他无关参数\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "        l = self.embedding(input_ids)\n",
        "        l_1 = self.pool(self.cnn_layer1(l))\n",
        "        l_2 = self.pool(self.cnn_layer2(l))\n",
        "        l_3 = self.pool(self.cnn_layer3(l))\n",
        "\n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training=training)  # training 需显式传递\n",
        "        model_output = self.last_dense(concatenated)\n",
        "\n",
        "        return model_output\n",
        "\n"
      ],
      "metadata": {
        "id": "NY3qZ8ut1462"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m])\n",
        "cnn.fit(train_data, epochs=10, validation_data=test_data, class_weight={1:4, 0:1}, callbacks=[F1ScoreCallback()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvm6ZNJD3k6q",
        "outputId": "53b5522f-1877-4969-af79-9984ef635f3c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7100 - f1_m: 0.4388 - loss: 1.9583Epoch 1 - Loss: 2.1422, Accuracy: 0.7414, Val Loss: 1.5665, Val Accuracy: 0.8564, F1-score: 0.2166\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 105ms/step - accuracy: 0.7102 - f1_m: 0.4378 - loss: 1.9591 - val_accuracy: 0.8564 - val_f1_m: 0.0227 - val_loss: 1.5665\n",
            "Epoch 2/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6025 - f1_m: 0.3229 - loss: 5.2301Epoch 2 - Loss: 2.4540, Accuracy: 0.7381, Val Loss: 1.8831, Val Accuracy: 0.8564, F1-score: 0.2028\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 109ms/step - accuracy: 0.6031 - f1_m: 0.3223 - loss: 5.2174 - val_accuracy: 0.8564 - val_f1_m: 0.0227 - val_loss: 1.8831\n",
            "Epoch 3/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6316 - f1_m: 0.3827 - loss: 4.4152Epoch 3 - Loss: 1.9176, Accuracy: 0.7724, Val Loss: 2.3914, Val Accuracy: 0.8564, F1-score: 0.2258\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 107ms/step - accuracy: 0.6322 - f1_m: 0.3820 - loss: 4.4037 - val_accuracy: 0.8564 - val_f1_m: 0.0227 - val_loss: 2.3914\n",
            "Epoch 4/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8129 - f1_m: 0.4661 - loss: 3.2865Epoch 4 - Loss: 1.3466, Accuracy: 0.8636, Val Loss: 1.5676, Val Accuracy: 0.8543, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 103ms/step - accuracy: 0.8131 - f1_m: 0.4652 - loss: 3.2776 - val_accuracy: 0.8543 - val_f1_m: 0.0909 - val_loss: 1.5676\n",
            "Epoch 5/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9057 - f1_m: 0.4661 - loss: 0.9900Epoch 5 - Loss: 0.4938, Accuracy: 0.8979, Val Loss: 1.4977, Val Accuracy: 0.6850, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - accuracy: 0.9057 - f1_m: 0.4652 - loss: 0.9877 - val_accuracy: 0.6850 - val_f1_m: 1.0000 - val_loss: 1.4977\n",
            "Epoch 6/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9722 - f1_m: 0.4661 - loss: 0.1894Epoch 6 - Loss: 0.1338, Accuracy: 0.9687, Val Loss: 1.6049, Val Accuracy: 0.6629, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 104ms/step - accuracy: 0.9721 - f1_m: 0.4652 - loss: 0.1891 - val_accuracy: 0.6629 - val_f1_m: 1.0000 - val_loss: 1.6049\n",
            "Epoch 7/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9910 - f1_m: 0.4661 - loss: 0.0615Epoch 7 - Loss: 0.0589, Accuracy: 0.9851, Val Loss: 1.7600, Val Accuracy: 0.6593, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - accuracy: 0.9909 - f1_m: 0.4652 - loss: 0.0615 - val_accuracy: 0.6593 - val_f1_m: 1.0000 - val_loss: 1.7600\n",
            "Epoch 8/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9931 - f1_m: 0.4661 - loss: 0.0464Epoch 8 - Loss: 0.0402, Accuracy: 0.9893, Val Loss: 1.8708, Val Accuracy: 0.6736, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 105ms/step - accuracy: 0.9931 - f1_m: 0.4652 - loss: 0.0463 - val_accuracy: 0.6736 - val_f1_m: 1.0000 - val_loss: 1.8708\n",
            "Epoch 9/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9944 - f1_m: 0.4661 - loss: 0.0320Epoch 9 - Loss: 0.0285, Accuracy: 0.9934, Val Loss: 2.0457, Val Accuracy: 0.6679, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - accuracy: 0.9944 - f1_m: 0.4652 - loss: 0.0320 - val_accuracy: 0.6679 - val_f1_m: 1.0000 - val_loss: 2.0457\n",
            "Epoch 10/10\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9958 - f1_m: 0.4661 - loss: 0.0437Epoch 10 - Loss: 0.0282, Accuracy: 0.9947, Val Loss: 2.0579, Val Accuracy: 0.6671, F1-score: 0.2581\n",
            "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 107ms/step - accuracy: 0.9958 - f1_m: 0.4652 - loss: 0.0436 - val_accuracy: 0.6671 - val_f1_m: 1.0000 - val_loss: 2.0579\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d74204d9390>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}